{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StrategicalIT/AKL-Feb-2026/blob/main/Lab01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cae130-b662-46d8-9b14-da63c83f8b31",
      "metadata": {
        "id": "a6cae130-b662-46d8-9b14-da63c83f8b31"
      },
      "source": [
        "# LAB1: Using NVIDIA NIM API\n",
        "## Introducing NVIDIA's API catalog\n",
        "In this lab we are going to use Python to access various models from NVIDIA's API catalog. You can explore the catalog by opening [https://build.nvidia.com/](https://build.nvidia.com/) in your web browser.\n",
        "You can interact graphically with the models but in this workshop we will learn how to access them programatically. For this, you will need an API key.\n",
        "\n",
        "## Getting a key from NVIDIA\n",
        "<font color=\"red\"><b>NOTE:</b> You need to have an NVIDIA API key and save it as a Google Colab secret.  Do that before proceeding any further</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291b6d01-f5f5-4162-b695-fa7eb74e665a",
      "metadata": {
        "id": "291b6d01-f5f5-4162-b695-fa7eb74e665a"
      },
      "source": [
        "## NIM API Reference\n",
        "Another site worth mentioning is the API reference site. It can be accessed at [https://docs.api.nvidia.com/nim/reference/llm-apis](https://docs.api.nvidia.com/nim/reference/llm-apis). It shows the actual API endpoints exposed by the NIM for each model which is typically <b>POST /v1/chat/completions</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9189748d-cb87-4066-83ed-8843d49e7118",
      "metadata": {
        "id": "9189748d-cb87-4066-83ed-8843d49e7118"
      },
      "source": [
        "## Installing depencies and loading libraries\n",
        "The first step is to install the necessary libraries. In this case we will install the openai Python library. This is considered the de-facto industry standard and most providers including NVIDIA NIM use the same API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a",
      "metadata": {
        "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a89838-296d-4b97-a91f-97552c4a5d0b",
      "metadata": {
        "id": "14a89838-296d-4b97-a91f-97552c4a5d0b"
      },
      "source": [
        "Now we can import the component we need for this lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7",
      "metadata": {
        "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d192363-ae4e-4d4a-9bad-998b7aed54b9",
      "metadata": {
        "id": "8d192363-ae4e-4d4a-9bad-998b7aed54b9"
      },
      "source": [
        "Next we read the key from the environment and store it in a variable called \"apikey\" for future use. You can \"uncomment\" the \"print\" command if you want to validate that it has been read correctly. Comments are created in Python by adding a #\n",
        "We'll also set a variable for the model we're using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b8d2b2-df87-4bbd-8227-3e9ec2f6571d",
      "metadata": {
        "id": "b5b8d2b2-df87-4bbd-8227-3e9ec2f6571d"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#apikey = os.environ[\"NVIDIA_API_KEY\"]\n",
        "#change from OS variable import to using Google Colab secret\n",
        "from google.colab import userdata\n",
        "apikey = userdata.get('apikey')\n",
        "#print(apikey)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll set a default model.  Start with llama-3.2-3b-instruct, you can come back and change this once you complete the lab and play around to see different results.\n",
        "Make sure only 1 line is setting a default i.e. all other default model lines shoudl be commented out with a #"
      ],
      "metadata": {
        "id": "KoZ7CwSPZfif"
      },
      "id": "KoZ7CwSPZfif"
    },
    {
      "cell_type": "code",
      "source": [
        "#set the model\n",
        "default_model = \"meta/llama-3.2-3b-instruct\"\n",
        "#default_model = \"meta/llama-3.1-405b-instruct\"\n",
        "print(default_model)"
      ],
      "metadata": {
        "id": "aDH2sKT2ZyPi"
      },
      "id": "aDH2sKT2ZyPi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3aa5d820-c444-4985-839d-a0c43c8509f7",
      "metadata": {
        "id": "3aa5d820-c444-4985-839d-a0c43c8509f7"
      },
      "source": [
        "## Getting our first completion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325",
      "metadata": {
        "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325"
      },
      "source": [
        "Let's create a client instance. This client will be able to access all models. No need for a separate client connection for each model.\n",
        "Notice how were we are using the \"apikey\" variable. The alternative would be to put your key wrapped in double quotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9",
      "metadata": {
        "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = apikey\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ffb2f6-14f4-4f4b-827d-a6e17b1c4d5a",
      "metadata": {
        "id": "61ffb2f6-14f4-4f4b-827d-a6e17b1c4d5a"
      },
      "source": [
        "<b>IMPORTANT</b>: NVIDIA is hosting the NIMs and exposing them via REST API. However, the same NIMs are available for download as containers. In that case the only change to the code here is the \"base_url\" parameter which will point to the NIM running in your Kubernetes cluster. When running NIM's locally the \"api_key\" parameter can be set to anything"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43eb6d62-2a79-4c55-8c55-9388f1574756",
      "metadata": {
        "id": "43eb6d62-2a79-4c55-8c55-9388f1574756"
      },
      "source": [
        "Finally, we can send our prompt to a model by using the client we just created. This is done with the \"chat.completions.create\" method which mimics OpenAI's API <br>\n",
        "Notice the following:\n",
        "- we are requesting a specific model with model=default_model\n",
        "- syntax of message \"role\", \"content\". Role can be \"user\" or \"system\"\n",
        "- parameters that modify the behavior of the model.\n",
        "  - temperature\" controls how random the LLM is when it generates text. Lower temperatures will produce more predictable and deterministic responses. Conversely, higher temperatures will yield more creative responses\n",
        "  - max_tokens is number of tokens the LLM can process in a single operation and it includes both the input and output tokens. This is a way of controlling the performance requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33b3df4-ddf8-45b2-8091-3a1f11e2b871",
      "metadata": {
        "id": "e33b3df4-ddf8-45b2-8091-3a1f11e2b871"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=default_model,\n",
        "  messages=[\n",
        "      {\n",
        "          \"role\":\"user\",\n",
        "          \"content\":\"Write a limerick about the wonders of GPU computing.\"\n",
        "      }\n",
        "  ],\n",
        "  temperature=0.2,\n",
        "  max_tokens=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26d7fd3-b537-4b67-8589-4b96b7bccb1f",
      "metadata": {
        "id": "f26d7fd3-b537-4b67-8589-4b96b7bccb1f"
      },
      "source": [
        "The completion variable contains the entire response from the model. In the next command we show only the actual content of the response message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a2cd4a-8e94-41b6-9a48-3f4faef53be3",
      "metadata": {
        "id": "a1a2cd4a-8e94-41b6-9a48-3f4faef53be3"
      },
      "outputs": [],
      "source": [
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fec9a74-08ca-467b-8f11-6cbb3cca8bc2",
      "metadata": {
        "id": "8fec9a74-08ca-467b-8f11-6cbb3cca8bc2"
      },
      "source": [
        "## Streaming response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8a69fd-f1aa-42b5-98fe-59e3c754d86d",
      "metadata": {
        "id": "1f8a69fd-f1aa-42b5-98fe-59e3c754d86d"
      },
      "source": [
        "In the previous code we are waiting for the whole output to be generated before we displayed in the screen. However, for use cases where the user is having a live interaction with the system is better to stream the tokens as they are generated. If the words are generated faster than a human can read them (approx 5 words per second) the experience will be satisfactory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134deb68-bf09-454b-8863-3d9f056cd5af",
      "metadata": {
        "id": "134deb68-bf09-454b-8863-3d9f056cd5af"
      },
      "source": [
        "Notice how we are setting the \"stream\" flag and then extracting the content from each of the different completion chunks as they come."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7370131d-fe54-4d4c-81bd-5db819988447",
      "metadata": {
        "id": "7370131d-fe54-4d4c-81bd-5db819988447"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=default_model,\n",
        "  messages=[{\"role\":\"user\",\"content\":\"Write a limerick about the wonders of GPU computing.\"}],\n",
        "  temperature=0.2,\n",
        "  top_p=0.7,\n",
        "  max_tokens=1024,\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "  if chunk.choices[0].delta.content is not None:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcc9b065-799e-44fc-b7f5-bada41bb68cf",
      "metadata": {
        "id": "bcc9b065-799e-44fc-b7f5-bada41bb68cf"
      },
      "source": [
        "## Ideas to explore further"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29101dec-5f36-4b6c-92f0-e511f0a836c9",
      "metadata": {
        "id": "29101dec-5f36-4b6c-92f0-e511f0a836c9"
      },
      "source": [
        "To finish this lab you can experiment with other models. Try using the same prompt with different models and compare the quality of the response. Some model suggestions:\n",
        "- meta/llama-3.1-8b-instruct\n",
        "- meta/llama-3.1-405b-instruct\n",
        "- deepseek-ai/deepseek-r1\n",
        "\n",
        "Things to observe:\n",
        "- You will notice how larger models have a larger latency\n",
        "- LRM's or Large Reasoning Models like Deepseek-r1 generate a lot of reasoning tokens. See all the \"reasoning\" Deepseek R1 does while creating the little poem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5359022f-c09d-4634-90d9-a9fc39e07c39",
      "metadata": {
        "id": "5359022f-c09d-4634-90d9-a9fc39e07c39"
      },
      "source": [
        "## End of Lab1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}