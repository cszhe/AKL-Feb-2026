{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StrategicalIT/AKL-Feb-2026/blob/main/Lab07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cae130-b662-46d8-9b14-da63c83f8b31",
      "metadata": {
        "id": "a6cae130-b662-46d8-9b14-da63c83f8b31"
      },
      "source": [
        "# LAB7: Using Nvidia NIM API with LlamaIndex\n",
        "In this lab we are going to use the LlamaIndex framework to interact with models and in particular with models from NVIDIA's API catalog. For this, you will need an API key for NVIDIA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2562424f-17b3-4db8-b4aa-0163a70900d4",
      "metadata": {
        "id": "2562424f-17b3-4db8-b4aa-0163a70900d4"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9189748d-cb87-4066-83ed-8843d49e7118",
      "metadata": {
        "id": "9189748d-cb87-4066-83ed-8843d49e7118"
      },
      "source": [
        "The first step is to install the necessary libraries. This installs the core llama-index package which draws a lot of dependencies.\n",
        "\n",
        "If you receive an error message about previous runtime versions (in Google Colab), click restart in the error message that pops up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a",
      "metadata": {
        "id": "fb3dbdb8-2c89-40b9-a1a5-532401e9ec5a"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e15b7a-6bd7-4e72-b7dd-e67896a57aca",
      "metadata": {
        "id": "62e15b7a-6bd7-4e72-b7dd-e67896a57aca"
      },
      "source": [
        "If you look carefully at the previous output you will notice that the only llm interface that has been installed is OpenAI. We are going to use NVIDIA NIMs so we need to install that module as well. You can see what LLM modules are available in [https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79ddf20-3824-4419-88ec-9e3b8c8ae63a",
      "metadata": {
        "id": "e79ddf20-3824-4419-88ec-9e3b8c8ae63a"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a89838-296d-4b97-a91f-97552c4a5d0b",
      "metadata": {
        "id": "14a89838-296d-4b97-a91f-97552c4a5d0b"
      },
      "source": [
        "Now we can import the components we need for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7",
      "metadata": {
        "id": "6ae207cd-f93f-48c5-90ef-ab838e012fe7"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.nvidia import NVIDIA\n",
        "from llama_index.core.llms import ChatMessage, MessageRole"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8357cefc-0bbd-4a8b-b55a-9f7b0a3dce02",
      "metadata": {
        "id": "8357cefc-0bbd-4a8b-b55a-9f7b0a3dce02"
      },
      "source": [
        "## Connect to the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab10fe8c-7f60-4c87-846d-90794407929e",
      "metadata": {
        "id": "ab10fe8c-7f60-4c87-846d-90794407929e"
      },
      "source": [
        "Next we read the key from the environment and store it in a variable called \"apikey\" for future use. You can uncomment the \"print\" command if you want to validate that it has been read correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfab7da-0ee0-4df7-94ae-42f9a9dcbd6d",
      "metadata": {
        "id": "bbfab7da-0ee0-4df7-94ae-42f9a9dcbd6d"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#apikey = os.environ[\"NVIDIA_API_KEY\"]\n",
        "#change from OS variable import to using Google Colab secret\n",
        "from google.colab import userdata\n",
        "apikey = userdata.get('apikey')\n",
        "#print(apikey)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325",
      "metadata": {
        "id": "5202b6d7-7643-44c1-9227-6ed9a60bb325"
      },
      "source": [
        "Let's create a client or LLM instance. As before, it points to the NVIDIA API and uses the API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9",
      "metadata": {
        "id": "b46582ab-3ecc-433f-8533-e4aa557ed1b9"
      },
      "outputs": [],
      "source": [
        "llm = NVIDIA(\n",
        "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "  api_key = apikey,\n",
        "  model=\"mistralai/mistral-7b-instruct-v0.2\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca8be7e-b368-4a8f-a046-52a9af901202",
      "metadata": {
        "id": "8ca8be7e-b368-4a8f-a046-52a9af901202"
      },
      "source": [
        "We can verify what model we are pointing to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628b9eb6-ec8d-498e-bc64-acb723dffdca",
      "metadata": {
        "id": "628b9eb6-ec8d-498e-bc64-acb723dffdca"
      },
      "outputs": [],
      "source": [
        "print(\"... Using: \", llm.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a8fe45-b4c1-4af4-99b6-a0d33e9df609",
      "metadata": {
        "id": "c8a8fe45-b4c1-4af4-99b6-a0d33e9df609"
      },
      "source": [
        "## Chat with the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee8bce9-e03f-494d-832c-a9827084d0bd",
      "metadata": {
        "id": "bee8bce9-e03f-494d-832c-a9827084d0bd"
      },
      "source": [
        "For a \"chat\" we need a list of messages that follow the \"role/content\" structure.\n",
        "- To define the system prompt you can use \"role=MessageRole.SYSTEM\". The system prompt is typically used to define the general expected behaviour of the system\n",
        "- The actual user prompt is defined with  \"role=MessageRole.USER\". This is the actual question from the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4260f76-1fb5-4e53-ac65-5f27294c34e1",
      "metadata": {
        "id": "f4260f76-1fb5-4e53-ac65-5f27294c34e1"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=MessageRole.SYSTEM,\n",
        "        content=(\"You are a helpful assistant.\")\n",
        "    ),\n",
        "    ChatMessage(\n",
        "        role=MessageRole.USER,\n",
        "        content=(\"What are the most popular house pets in Australia?\"),\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43eb6d62-2a79-4c55-8c55-9388f1574756",
      "metadata": {
        "id": "43eb6d62-2a79-4c55-8c55-9388f1574756"
      },
      "source": [
        "Finally, we can send our prompt to the model. Notice this is as simple as \"llm.chat\". This will be the same syntax for any of the LLM modules that are available in LlamaIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33b3df4-ddf8-45b2-8091-3a1f11e2b871",
      "metadata": {
        "id": "e33b3df4-ddf8-45b2-8091-3a1f11e2b871"
      },
      "outputs": [],
      "source": [
        "response = llm.chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a2cd4a-8e94-41b6-9a48-3f4faef53be3",
      "metadata": {
        "id": "a1a2cd4a-8e94-41b6-9a48-3f4faef53be3"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b971eab-59be-43d4-acc5-6fe53e8b001f",
      "metadata": {
        "id": "7b971eab-59be-43d4-acc5-6fe53e8b001f"
      },
      "source": [
        "## Streaming chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8a69fd-f1aa-42b5-98fe-59e3c754d86d",
      "metadata": {
        "id": "1f8a69fd-f1aa-42b5-98fe-59e3c754d86d"
      },
      "source": [
        "If we want a streaming output we can use this instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c3b3f5-4666-47a3-b050-3d55552ceae6",
      "metadata": {
        "id": "d7c3b3f5-4666-47a3-b050-3d55552ceae6"
      },
      "outputs": [],
      "source": [
        "response = llm.stream_chat(messages)\n",
        "for r in response:\n",
        "    print(r.delta, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134deb68-bf09-454b-8863-3d9f056cd5af",
      "metadata": {
        "id": "134deb68-bf09-454b-8863-3d9f056cd5af"
      },
      "source": [
        "On the other hand if we only need a single \"completion\" instead of an interactive \"chat\", we can use the following. Notice how we don't need to pass the role of the message, just the message itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7370131d-fe54-4d4c-81bd-5db819988447",
      "metadata": {
        "id": "7370131d-fe54-4d4c-81bd-5db819988447"
      },
      "outputs": [],
      "source": [
        "response = llm.complete(\"What are the most popular house pets in New Zealand?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5359022f-c09d-4634-90d9-a9fc39e07c39",
      "metadata": {
        "id": "5359022f-c09d-4634-90d9-a9fc39e07c39"
      },
      "source": [
        "## End of Lab7"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}